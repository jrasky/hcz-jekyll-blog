<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jerome Rasky</title>
    <description>Personal Website</description>
    <link>http://rasky.co//</link>
    <atom:link href="http://rasky.co//sitemap.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 28 Oct 2017 10:14:13 -0700</pubDate>
    <lastBuildDate>Sat, 28 Oct 2017 10:14:13 -0700</lastBuildDate>
    <generator>Jekyll v3.6.2</generator>
    
      <item>
        <title>Software Design Patterns</title>
        <description>&lt;p&gt;Software is full of design patterns. What I’ve come to realize over the course of the last summer is that function of software can be split along somewhat generic lines. These steps are not extremely clear cut, nor is it possible to strictly categorize every line this way. The lesson here is that thinking along these lines is a productive way to consider the organization of the software that you write.&lt;/p&gt;

&lt;p&gt;This is inspired by &lt;a href=&quot;https://www.destroyallsoftware.com/&quot;&gt;Destroy All Software&lt;/a&gt;’s talk called &lt;a href=&quot;https://www.destroyallsoftware.com/talks/boundaries&quot;&gt;Boundaries&lt;/a&gt;, which presents the concept of functional core, imperative shell. I have found this model very useful, but Gary Bernhardt himself does not have an answer for how to shape the imperative shell. By building software along the lines of imperative shell, functional core, I have come to realize these distinctions below. Following these steps gives a clearer shape to the imperative shell, and clarifies the exact role of the functional core.&lt;/p&gt;

&lt;p&gt;These are probably not new ideas, but they are ideas that I have found specifically helpful in considering the design of software components.&lt;/p&gt;

&lt;p&gt;More than these principles, having enough time to develop software is critical. Good design choices do not lend themselves well to expediency, and chances are if you are feeling time pressure your priorities will shift and your code will suffer no matter what. Getting your sprints under control is absolutely the best way to improve software quality. Once you have reached a sustainable pace, these steps below may become useful.&lt;/p&gt;

&lt;h2 id=&quot;determines-input&quot;&gt;Determines Input&lt;/h2&gt;
&lt;p&gt;Software often starts by determining the input provided by the user. Obtaining this input is a fundamentally imperative task that involves modifying global state. Transactions are dependent on the will of the user, so user input must be determined before a transaction can be built.&lt;/p&gt;

&lt;h2 id=&quot;determines-state&quot;&gt;Determines State&lt;/h2&gt;
&lt;p&gt;Software then determines the current state of the system. User commands typically involve some kind of stateful transaction against the current state, such as “copy file.” Before a decision can be made about what kind of transaction to construct, the current state needs to be available.&lt;/p&gt;

&lt;h2 id=&quot;builds-transaction&quot;&gt;Builds Transaction&lt;/h2&gt;
&lt;p&gt;With the user input and the current state gathered, the software can now make decisions. These decisions involve considering the current state and the user input, in order to determine what kind of transaction to build, and what parameters to give to that transaction. This part of the code is functional, since it maps input state onto an output transaction, without otherwise invoking any side effects.&lt;/p&gt;

&lt;h2 id=&quot;executes-transaction&quot;&gt;Executes Transaction&lt;/h2&gt;
&lt;p&gt;With the transaction now built, the software can take its parameters and kind and invoke side effects to execute the transaction. There may also be error handling here, or other sub-patterns of this kind for cases such as rollback. Software does not make decisions here, other than directly from the parameters of the transaction.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Most software resembles the steps above. Or, at least, the above steps can be extracted out of many pieces of software. Badly written software will often mix these steps together, making it very tough to test each part of the process separately. Following this philosophy of separation will split your software into components that are easier to reason about and test. By cleanly separating decisions from side effects, integration tests can be fewer and simpler, with faster functional tests picking up the slack instead.&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Sep 2017 00:00:00 -0700</pubDate>
        <link>http://rasky.co//2017/09/02/software-design-patterns.html</link>
        <guid isPermaLink="true">http://rasky.co//2017/09/02/software-design-patterns.html</guid>
        
        
      </item>
    
      <item>
        <title>Android Performance</title>
        <description>&lt;p&gt;Despite their impressive hardware specifications, Android devices have an unfortunate reputation of poor performance. Frames get dropped, launching an application takes a long time, or the whole device seems to freeze. This kind of unpredictable performance isn’t great. One important factor of any platform is the programming language used. Android is based around Java, a dynamic, garbage-collected language with a hefty runtime and complex features that are tough to optimize. Android’s kernel, Linux, is designed more for servers than smartphones, and amplifies performance issues by making poor use of hardware. The story of Android performance follows the design of the platform, from the Java compiler and runtime to the Linux kernel at the base.&lt;/p&gt;

&lt;p&gt;The Java Virtual Machine on Android was originally designed to be simple, to conform to the resource constraints of the hardware it was running on. Google chose to create a memory-efficient bytecode and an interpreter for their JVM. Over time, hardware improved, and the JVM shifted to become a Just-In-Time compiler. JIT compilers are fast, and avoid complex optimizations. Regardless, compiler overhead meant application startup time became problem. Google tackled this by designing an ahead-of-time compiler to supplement the existing JIT design. By shifting the work of the compiler from application startup to installation, startup time improved without sacrificing performance.&lt;/p&gt;

&lt;p&gt;Garbage collection is about shifting the cost of memory management to reduce latency. First generation hardware limitations required choosing a design for the garbage collector that performed poorly on newer hardware. Applications often had to wait on the garbage collector, at busy times that made this pausing very noticeable. The new JIT design for the JVM improved on these glitches, introducing concurrent garbage collection and compaction. Those and other more complex strategies mean that modern applications rarely pause to wait for garbage collection.&lt;/p&gt;

&lt;p&gt;Between ahead-of-time compilation and improved garbage collection, the new Android JVM accomplishes the important goal of getting out of the way. There are some remaining performance issues, but most are solved. Some applications have worse performance owing to their own design, which will always be the case on any platform.&lt;/p&gt;

&lt;p&gt;The JVM’s design had matured, but issues around latency persisted, especially audio latency. Gaps in audio playback are even more unpleasant than dropped frames, and cause loud pops. Google’s documentation details how the default scheduler in Linux prioritizes threads that have not used processor time recently. This means that despite giving priority to UI and audio threads, other tasks might preempt user-facing ones during periods of high system usage. The solution for audio threads is real-time scheduling, so that they will always run immediately if they are ready.&lt;/p&gt;

&lt;p&gt;Google has recently announced that Android “O” will carry hard limits on background task execution. This will improve the situation considerably, as applications will no longer be as able to impact performance once they aren’t open. Google’s other project, Fuschia, is a complete redesign of the kernel, and represents the more nuclear approach to the performance problem in Android. It will be interesting to see what solutions are in store in the future, and what shape the Android operating system and ecosystem will take.&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Jun 2017 00:00:00 -0700</pubDate>
        <link>http://rasky.co//2017/06/09/android-performance.html</link>
        <guid isPermaLink="true">http://rasky.co//2017/06/09/android-performance.html</guid>
        
        
      </item>
    
      <item>
        <title>Why Concurrency doesn't Matter</title>
        <description>&lt;p&gt;Or, why I like Rust&lt;/p&gt;

&lt;p&gt;There’s a lot of talk these days about how important concurrency is. The reasons
are obvious: in a world of computing at scale, single processors don’t. Many
people decry the death of Moore’s law, and increasingly they seem to be right.
Transistors can only get so small, after all. As a result, the constantly
increasing desire for computer performance will have to come from concurrency,
working around the issue of Moore’s law entirely. All of this is sound reasoning.&lt;/p&gt;

&lt;p&gt;Most older languages were designed in a world where Moore’s law was still very
valid, and where compute performance came from single processors, not farms of
them. Concurrency and scale networks existed, but were largely restricted to
scientific computing or very special edge-cases, and the technologies that came
out of them were not generally useful. Those days were great. However, that
world doesn’t exist anymore, and instead the lessons being drawn from the
current flock of “giants” is the importance of scale. The future is now, and it
looks like networks.&lt;/p&gt;

&lt;p&gt;As a result, these older languages did not serve the concurrency story very
well. Though concurrency primitives existed in those languages, they usually
require careful thought and planning for effective use. Data races abound,
memory corruption is easy. In general, it’s not the most inviting environment.
In reaction to this, certain modern languages tout “concurrency support” as a
major feature. The goal: to be rid of the pitfalls of traditional concurrency,
and instead present a sane, easy-to-use interface that falls neatly in with all
the other traditional paradigms of programming everybody is used to. Truly a
noble cause.&lt;/p&gt;

&lt;p&gt;The problem lies here: concurrency is an anti-pattern to the traditional
programming paradigms. Computing takes a certain level of discipline, especially
with non-managed languages. Concurrency makes everything much harder, requiring
either a much higher degree of discipline or a much higher degree of management.
The latter group includes languages like Haskell, which are generally much more
declarative. What you lose in terms of control over your code, however, you gain
in performance. Suddenly, all the traditional problems with concurrency go away,
and you’re left with something concurrent and safe. This is awesome, except that
you lose many of the traditional paradigms most programmers are used to. It’s
great for a wide variety of problems that benefit greatly from concurrency, but
it’s also hard to get used to, and not great for an equally large number of
corner cases in more traditional programming where a certain level of
concurrency plays positively to the application. Not only this, but learning
this style requires a lot of domain-specific work and knowledge. This doesn’t
sound like the best solution.&lt;/p&gt;

&lt;p&gt;What certain modern languages offer does sound like a great solution: sane
concurrency without any of the micro-management baggage. It’s a great promise,
but one that’s ultimately contradictory. These modern languages do provide great
front-ends to the concurrency primitives everybody loves to hate. If your
greatest worry is about correctly creating or scheduling threads, or scaling to
tens of thousands of threads, or about correctly cleaning up a thread’s
resources when it finishes, then these new languages offer exactly what you’re
after.&lt;/p&gt;

&lt;p&gt;That’s not really the–or, at least, my–greatest concern with threads.
Obviously, resource management and scale are an issue, but those are whenever
I’m doing any programming. They’re amplified by threads, but they aren’t
fundamentally &lt;em&gt;new&lt;/em&gt; worries. The new worries I do have relate to data races.
Threading means giving up a huge amount of control over how your code executes,
which means any interactions different parts of your code have are now much less
predictable. The easy way to get out of this trap is to prevent all
interactions, but that precludes any real benefits that concurrency provides. If
all concurrent tasks are completely independent, then they cannot service the
same problem.&lt;/p&gt;

&lt;p&gt;Using concurrency to work on a single problem requires sharing certain
resources, even if those resources are simply messages. That shared state means
data races are possible. Data races are an entirely different class of worries
from what’s possible in traditional, single-threaded programming. These concerns
are serviced by the micro-management model, but not by the sexy-interface model
employed by certain modern languages. As a result, these new languages do not
really solve any underlying problems, but instead lather on a fresh layer of
paint. On a small scale, everything feels awesome. On a larger scale, all the
same problems come back.&lt;/p&gt;

&lt;p&gt;This is why concurrency doesn’t matter. It doesn’t because the real problems
concurrency causes aren’t addressed by modern languages. Certain other, more
declarative languages address them to a certain degree, but are “weird” enough
to not be generally useful. Or, if they are, they haven’t seen very much
adoption.&lt;/p&gt;

&lt;p&gt;What about the resource management and scaling category of problems, however?
Although they are fairly general, they are definitely amplified by concurrency.
The traditional answer to resource management has been garbage collection, which
is a good solution, but doesn’t &lt;em&gt;always&lt;/em&gt; scale very well. Not only this, but
resource management incurs a performance penalty, caused by the garbage
collection itself and by the extra work required when acquiring or releasing
resources. Much like the micro-management model for concurrency, this doesn’t
seem like the best solution.&lt;/p&gt;

&lt;p&gt;This is where Rust comes in. Rust’s major innovation is the borrow checker, an
additional bit of information the compiler tracks which enables it to determine
when data comes into and falls out of scope. This means the compiler can
identify in a deterministic fashion when to free resources, meaning it can
insert this information into the binary it produces. No more need for separate
garbage collection, instead resources are freed at the ends of certain lexical
scopes. This can obviously hurt performance–especially when code is written
such that large amounts of data are copied and freed–but this is always the
case when the language does not micro-manage your code.&lt;/p&gt;

&lt;p&gt;What isn’t always the case is the level of control that a borrow checker gives
you. Even if the worst case isn’t very different from a garbage collector, the
average case is much better. All the resources your program uses are right
there, instead of being hidden away behind a garbage collector. Plus, the
behavior is defined by the lexical scope of your code, and not by some black-box
mechanics taking place behind the scenes of the garbage collector.&lt;/p&gt;

&lt;p&gt;All of this makes for a mechanic that I think will survive Rust as a language.
The borrow checker is something extremely useful in general programming, not
only because it manages resources with no overhead, but because it also prevents
many classes of memory corruption issues and data races often present in other
languages. Much like type checking before it, I think borrow checking will
become part of future languages and we’ll all wonder how we ever dealt with
programming without it.&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Aug 2015 00:00:00 -0700</pubDate>
        <link>http://rasky.co//2015/08/21/why-concurrency-doesnt-matter.html</link>
        <guid isPermaLink="true">http://rasky.co//2015/08/21/why-concurrency-doesnt-matter.html</guid>
        
        
      </item>
    
      <item>
        <title>The New MacBook</title>
        <description>&lt;p&gt;The new MacBook is pretty cool. This morning, when I was watching the liveblog, their announcement of it being fanless made me immediately suspect that it would be ARM-based. I’m impressed by Intel, and their ability to bring the TDP down enough to not require a fan.&lt;/p&gt;

&lt;p&gt;I’m similarly excited about Type C. It’s basically the god-cable that we’ve all always wanted: one that handles power delivery as well as bidirectional, high-bandwidth data delivery. It really fills all the connectivity needs of the laptop, except for analog output.&lt;/p&gt;

&lt;p&gt;That being said, it is priced rather high. I know the overall quality of the components is to blame, but it would be nice to have it either be more on par with the performance of the Pro, or more on the price level of the Air.&lt;/p&gt;

&lt;p&gt;As an aside, I’m somewhat fearful as the trend of computing is to move to more “secured” devices, and other technologies that have the end result of making it harder to run the OS of my choosing on my computers. I fear that one day I may be restricted to a very large degree on most devices.&lt;/p&gt;

&lt;p&gt;I could completely deal with OS X or Windows; both are very good operating systems. The problem is their software packaging: since both are somewhat stable, and applications are more self-contained, everything just ships statically linked, giant binaries. Plus, neither operating system has anything nearing APT, making software management much harder than it has to be. Yes, OS X has the App Store, but it is missing most of the applications that I use on a regular basis.&lt;/p&gt;

&lt;p&gt;This, plus desktop Linux is going to be very exciting in the next few years. KDBus is going to land soon, as well as Wayland/Mir/others. Valve is getting serious with SteamOS, and btrfs/systemd/containers are promising some very interesting things in the system management and software packaging areas. It’s just the right time to be using Linux, and having that taken away from me would make me sad.&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Mar 2015 00:00:00 -0700</pubDate>
        <link>http://rasky.co//2015/03/09/the-new-macbook.html</link>
        <guid isPermaLink="true">http://rasky.co//2015/03/09/the-new-macbook.html</guid>
        
        
      </item>
    
      <item>
        <title>Working with the Terminal</title>
        <description>&lt;p&gt;Having done by now a fair bit of work building a terminal application in Linux,
I’d like to mention how much the interface matches what I’d like to call “the
text box of doom.” There are a few major issues that I have with it, namely&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;No way to read out the contents of the screen&lt;/p&gt;

    &lt;p&gt;I understand the issue with doing so, but there being no way of getting the
 contents of the current screen means there is no real way to know what is on
 there–even if you clear everything and redraw, there might be some quirks
 with the terminal emulator that cause it to not actually contain what you
 think it contains.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;No strict separation of control and text sequences&lt;/p&gt;

    &lt;p&gt;Again, the limitations of the serial connection mean escape characters are
 really the only way to go when designing control sequences, but it would be
 nice to not mix them in with the text. I understand the simplicity of having
 applications just be able to print out contents and read back lines, but
 given the existence of termios this shouldn’t need to be. Instead, there
 should be a “high” mode where text is a control sequence.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Output at the end of the line counts as a newline&lt;/p&gt;

    &lt;p&gt;Ideally, lines should be inputted as that: lines. At least, there should be
 a way to do so, much like there is a line input method on the other end.
 Applications shouldn’t have to worry about how long a line is exactly, and
 instead the terminal should treat each line like a line.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The result of all this is a lot of work and redundancy on the part of the
application to maintain a correct representation of the state of the display,
something which a good API should specifically avoid.&lt;/p&gt;
</description>
        <pubDate>Fri, 06 Mar 2015 00:00:00 -0800</pubDate>
        <link>http://rasky.co//2015/03/06/working-with-the-teminal.html</link>
        <guid isPermaLink="true">http://rasky.co//2015/03/06/working-with-the-teminal.html</guid>
        
        
      </item>
    
  </channel>
</rss>
